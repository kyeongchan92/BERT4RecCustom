{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyeongchanlee/.pyenv/versions/3.9.9/envs/candidate/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset.dataset import ML1MDataset\n",
    "from dataloader.dataloader import BertDataloader, BertEvalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from options import args\n",
    "dataset = ML1MDataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = dataset._get_preprocessed_dataset_path()\n",
    "if not dataset_path.parent.is_dir():\n",
    "    dataset_path.parent.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_code's return : ml-1m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyeongchanlee/PycharmProjects/BertCustom/BertCustom/dataset/dataset.py:61: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(file_path, sep='::', header=None)\n"
     ]
    }
   ],
   "source": [
    "df = dataset.load_ratings_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning into implicit ratings\n"
     ]
    }
   ],
   "source": [
    "df = dataset.make_implicit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering triplets\n"
     ]
    }
   ],
   "source": [
    "df = dataset.filter_triplets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Densifying index\n"
     ]
    }
   ],
   "source": [
    "df, u_i_map = dataset.densify_index(df, target_cols=['sid', 'uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>sid</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1105</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>854</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3178</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2163</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000204</th>\n",
       "      <td>6040</td>\n",
       "      <td>1020</td>\n",
       "      <td>1</td>\n",
       "      <td>956716541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000205</th>\n",
       "      <td>6040</td>\n",
       "      <td>1023</td>\n",
       "      <td>5</td>\n",
       "      <td>956704887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000206</th>\n",
       "      <td>6040</td>\n",
       "      <td>549</td>\n",
       "      <td>5</td>\n",
       "      <td>956704746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000207</th>\n",
       "      <td>6040</td>\n",
       "      <td>1025</td>\n",
       "      <td>4</td>\n",
       "      <td>956715648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000208</th>\n",
       "      <td>6040</td>\n",
       "      <td>1026</td>\n",
       "      <td>4</td>\n",
       "      <td>956715569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000209 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          uid   sid  rating  timestamp\n",
       "0           1  1105       5  978300760\n",
       "1           1   640       3  978302109\n",
       "2           1   854       3  978301968\n",
       "3           1  3178       4  978300275\n",
       "4           1  2163       5  978824291\n",
       "...       ...   ...     ...        ...\n",
       "1000204  6040  1020       1  956716541\n",
       "1000205  6040  1023       5  956704887\n",
       "1000206  6040   549       5  956704746\n",
       "1000207  6040  1025       4  956715648\n",
       "1000208  6040  1026       4  956715569\n",
       "\n",
       "[1000209 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df.uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df.sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tra_val_tes(df, user_count):\n",
    "    if args.split == 'leave_one_out':\n",
    "        print('Splitting')\n",
    "        user_group = df.groupby('uid')\n",
    "        user2items = user_group.progress_apply(lambda u_df: list(u_df.sort_values(by='timestamp')['sid']))\n",
    "        # user2genres = user_group.progress_apply(lambda d: list(d.sort_values(by='timestamp')['gid']))\n",
    "\n",
    "        train, val, test = {}, {}, {}\n",
    "\n",
    "        # train_g, val_g, test_g = {}, {}, {}\n",
    "        for user in range(1, user_count+1):\n",
    "            items = user2items[user]\n",
    "            train[user], val[user], test[user] = items[:-2], items[-2:-1], items[-1:]\n",
    "\n",
    "            # genres = user2genres[user]\n",
    "            # train_g[user], val_g[user], test_g[user] = genres[:-2], genres[-2:-1], genres[-1:]\n",
    "\n",
    "        return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:01<00:00, 3655.47it/s]\n"
     ]
    }
   ],
   "source": [
    "tra_item_seq, val_item_seq, tes_item_seq = split_tra_val_tes(df, len(u_i_map['uid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_attrs_df(self):\n",
    "    '''\n",
    "    Return : \n",
    "        attr_df = itemid |   genre  |  ...\n",
    "                    1    | 'comedy' |\n",
    "    '''\n",
    "    attr_df = dataset.load_movies_df()\n",
    "    # attr_df = attr_df[attr_df['sid'].isin(rating_df['sid'])]\n",
    "    return attr_df[['sid'] + dataset.attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyeongchanlee/PycharmProjects/BertCustom/BertCustom/dataset/dataset.py:68: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(file_path, sep='::', header=None, encoding='ISO-8859-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_code's return : ml-1m\n"
     ]
    }
   ],
   "source": [
    "attrs_df = load_attrs_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_df['sid'] = attrs_df['sid'].map(u_i_map['sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>3702.0</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3879</th>\n",
       "      <td>3703.0</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>3704.0</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>3705.0</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3882</th>\n",
       "      <td>3706.0</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3883 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sid                         genre\n",
       "0        1.0   Animation|Children's|Comedy\n",
       "1        2.0  Adventure|Children's|Fantasy\n",
       "2        3.0                Comedy|Romance\n",
       "3        4.0                  Comedy|Drama\n",
       "4        5.0                        Comedy\n",
       "...      ...                           ...\n",
       "3878  3702.0                        Comedy\n",
       "3879  3703.0                         Drama\n",
       "3880  3704.0                         Drama\n",
       "3881  3705.0                         Drama\n",
       "3882  3706.0                Drama|Thriller\n",
       "\n",
       "[3883 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Densifying index\n"
     ]
    }
   ],
   "source": [
    "attrs_df, amap = dataset.densify_index(attrs_df, target_cols=dataset.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>3702.0</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3879</th>\n",
       "      <td>3703.0</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>3704.0</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>3705.0</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3882</th>\n",
       "      <td>3706.0</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3883 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sid  genre\n",
       "0        1.0    179\n",
       "1        2.0    239\n",
       "2        3.0     33\n",
       "3        4.0    150\n",
       "4        5.0    160\n",
       "...      ...    ...\n",
       "3878  3702.0    160\n",
       "3879  3703.0    254\n",
       "3880  3704.0    254\n",
       "3881  3705.0    254\n",
       "3882  3706.0    269\n",
       "\n",
       "[3883 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2attr_map = []\n",
    "for attr_name in dataset.attributes:\n",
    "    i2attr_map.append(dict(zip(attrs_df['sid'], attrs_df[attr_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dataset_path.parent.is_dir():\n",
    "    dataset_path.parent.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('Data/preprocessed/min_rating0-min_uc5-min_sc0-splitleave_one_out/dataset.pkl')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = dataset._get_preprocessed_dataset_path()\n",
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dataset_pickle = {'tra_item_seq': tra_item_seq,  # dict : {densified user's id : [3, 4, ...](densified item's id)}\n",
    "            'val_item_seq': val_item_seq,\n",
    "            'tes_item_seq': tes_item_seq,\n",
    "            'u_i_map' : u_i_map,\n",
    "            'i2attr_map' : i2attr_map,\n",
    "            'amap' : amap \n",
    "            }\n",
    "with dataset_path.open('wb') as f:\n",
    "    pickle.dump(dataset_pickle, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already preprocessed. Skip preprocessing\n",
      "Negatives samples exist. Loading.\n",
      "Negatives samples exist. Loading.\n"
     ]
    }
   ],
   "source": [
    "dataloader = BertDataloader(args, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader, val_loader, test_loader = dataloader.get_pytorch_dataloaders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already preprocessed. Skip preprocessing\n"
     ]
    }
   ],
   "source": [
    "prep_data = dataset.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = args.dataloader_random_seed\n",
    "rng = random.Random(seed)\n",
    "train = prep_data['tra_item_seq']\n",
    "val = prep_data['val_item_seq']\n",
    "test = prep_data['tes_item_seq']\n",
    "\n",
    "umap = prep_data['u_i_map']['uid']\n",
    "smap = prep_data['u_i_map']['sid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_count = len(umap)\n",
    "item_count = len(smap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_items = len(smap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3706"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = args.bert_max_len\n",
    "mask_prob = args.bert_mask_prob\n",
    "CLOZE_MASK_TOKEN = item_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2attr_map = prep_data['i2attr_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_attributes = args.use_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_each_size = [len(_map) for _map in amap.values()]\n",
    "args.attrs_each_size = attrs_each_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[301]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrs_each_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[302]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATTRS_MASK_TOKENS = [size+1 for size in attrs_each_size]\n",
    "ATTRS_MASK_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _map, m_token in zip(i2attr_map, ATTRS_MASK_TOKENS):\n",
    "    _map[CLOZE_MASK_TOKEN] = m_token  # item's mask token to attr's mask token\n",
    "    _map[0] = 0  # pad token too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2attr_map[0][3707]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negatives samples exist. Loading.\n",
      "Negatives samples exist. Loading.\n"
     ]
    }
   ],
   "source": [
    "from dataloader.negative_samplers import negative_sampler_factory\n",
    "save_folder = dataset._get_preprocessed_folder_path()\n",
    "\n",
    "code = args.train_negative_sampler_code\n",
    "train_negative_sampler = negative_sampler_factory(code, train, val, test,\n",
    "                                                    user_count, item_count,\n",
    "                                                    args.train_negative_sample_size,\n",
    "                                                    args.train_negative_sampling_seed,\n",
    "                                                    save_folder)\n",
    "code = args.test_negative_sampler_code\n",
    "test_negative_sampler = negative_sampler_factory(code, train, val, test,\n",
    "                                                    user_count, item_count,\n",
    "                                                    args.test_negative_sample_size,\n",
    "                                                    args.test_negative_sampling_seed,\n",
    "                                                    save_folder)\n",
    "\n",
    "train_negative_samples = train_negative_sampler.get_negative_samples()\n",
    "test_negative_samples = test_negative_sampler.get_negative_samples()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "class BertTrainDataset(data_utils.Dataset):\n",
    "    def __init__(self, u2seq, max_len, mask_prob, seq_mask_token, num_items, rng, i2attr_map, use_attributes):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.max_len = max_len\n",
    "        self.mask_prob = mask_prob\n",
    "        self.seq_mask_token = seq_mask_token\n",
    "        self.num_items = num_items\n",
    "        self.rng = rng\n",
    "\n",
    "        self.use_attributes = use_attributes\n",
    "        self.i2attr_map = i2attr_map\n",
    "\n",
    "        self.printer = PrintInputShape(3)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self.u2seq[user]\n",
    "\n",
    "        return user, seq\n",
    "\n",
    "        # tokens = []\n",
    "        # labels = []\n",
    "        # attrs = []\n",
    "\n",
    "        # for s in zip(seq):\n",
    "        #     prob = self.rng.random()\n",
    "        #     if prob < self.mask_prob:\n",
    "        #         prob /= self.mask_prob\n",
    "\n",
    "        #         if prob < 0.8:\n",
    "        #             tokens.append(self.seq_mask_token)\n",
    "        #         elif prob < 0.9:\n",
    "        #             tokens.append(self.rng.randint(1, self.num_items))\n",
    "        #         else:\n",
    "        #             tokens.append(s)\n",
    "\n",
    "        #         labels.append(s)\n",
    "        #     else:\n",
    "        #         tokens.append(s)\n",
    "        #         labels.append(0)\n",
    "\n",
    "        # tokens = tokens[-self.max_len:]\n",
    "        # labels = labels[-self.max_len:]\n",
    "\n",
    "        # mask_len = self.max_len - len(tokens)\n",
    "\n",
    "        # tokens = [0] * mask_len + tokens\n",
    "        # labels = [0] * mask_len + labels\n",
    "\n",
    "        # # print(tokens)\n",
    "        # if self.use_attributes:\n",
    "        #     for _map in i2attr_map:\n",
    "        #         attr_idx = torch.LongTensor([_map[item] for item in tokens])\n",
    "        #         attrs.append(attr_idx)\n",
    "\n",
    "        # return torch.LongTensor(tokens), torch.LongTensor(attrs), torch.LongTensor(labels)\n",
    "\n",
    "from utils import PrintInputShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainCollator:\n",
    "    def __init__(self, rng, seq_mask_token, num_items, max_len, use_attributes, i2attr_map):\n",
    "        self.rng = rng\n",
    "        self.seq_mask_token = seq_mask_token\n",
    "        self.num_items = num_items\n",
    "        self.max_len = max_len\n",
    "        self.use_attributes = use_attributes\n",
    "        self.i2attr_map = i2attr_map\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        # users_batch = [_[0] for _ in batch]\n",
    "        seqs_batch = [_[1] for _ in batch]\n",
    "\n",
    "        seqs_output = []\n",
    "        labels_output = []\n",
    "        attrs_output = []\n",
    "\n",
    "        for seq in seqs_batch:\n",
    "            one_seq = []\n",
    "            one_seq_labels = []\n",
    "            for item in seq:\n",
    "                prob = self.rng.random()\n",
    "                if prob < mask_prob:\n",
    "                    prob /= mask_prob\n",
    "\n",
    "                    if prob < 0.8:\n",
    "                        one_seq.append(self.seq_mask_token)\n",
    "                    elif prob < 0.9:\n",
    "                        one_seq.append(self.rng.randint(1, self.num_items))\n",
    "                    else:\n",
    "                        one_seq.append(item)\n",
    "\n",
    "                    one_seq_labels.append(item)\n",
    "                else:\n",
    "                    one_seq.append(item)\n",
    "                    one_seq_labels.append(0)\n",
    "\n",
    "            one_seq = one_seq[-self.max_len:]\n",
    "            one_seq_labels = one_seq_labels[-self.max_len:]\n",
    "\n",
    "            mask_len = max_len - len(one_seq)\n",
    "\n",
    "            one_seq = [0] * mask_len + one_seq\n",
    "            one_seq_labels = [0] * mask_len + one_seq_labels\n",
    "\n",
    "            seqs_output.append(one_seq)\n",
    "            labels_output.append(one_seq_labels)\n",
    "\n",
    "        if self.use_attributes:\n",
    "            for _map in self.i2attr_map:\n",
    "                one_map = []\n",
    "                for seq in seqs_output:\n",
    "                    one_map.append([_map[item] for item in seq])\n",
    "                    \n",
    "                attrs_output.append(one_map)  # 3d. [2d, 2d, ...] / 2d : one attribute\n",
    "        \n",
    "        return torch.LongTensor(seqs_output), torch.LongTensor(attrs_output), torch.LongTensor(labels_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BertTrainDataset(train, max_len, mask_prob, CLOZE_MASK_TOKEN,\n",
    "    item_count, rng, i2attr_map, use_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train_collator = MyTrainCollator(rng, CLOZE_MASK_TOKEN, item_count, max_len, use_attributes, i2attr_map)\n",
    "train_dataloader = data_utils.DataLoader(train_dataset, batch_size=args.train_batch_size,\n",
    "    shuffle=True, pin_memory=True, collate_fn=my_train_collator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEvalDataset(data_utils.Dataset):\n",
    "    def __init__(self, args, u2seq, u2answer, max_len, seq_mask_token, negative_samples, i2attr_map, use_attributes):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.u2answer = u2answer\n",
    "        self.max_len = max_len\n",
    "        self.seq_mask_token = seq_mask_token\n",
    "        self.negative_samples = negative_samples\n",
    "\n",
    "        self.i2attr_map = i2attr_map\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self.u2seq[user]\n",
    "\n",
    "        return user, seq\n",
    "        answer = self.u2answer[user]\n",
    "        # negs = self.negative_samples[user]\n",
    "        # attrs = []\n",
    "\n",
    "        # candidates = answer + negs\n",
    "        # labels = [1] * len(answer) + [0] * len(negs)\n",
    "        # seq = self.padding_and_trim(seq, 'seq')\n",
    "\n",
    "        # if self.use_attributes:\n",
    "        #     attr_idx = torch.LongTensor([self.i2attr_map[attr_name][item] for item in sequence])\n",
    "        #     attrs.append(attr_idx)\n",
    "\n",
    "        # return torch.LongTensor(seq), torch.LongTensor(attrs), torch.LongTensor(candidates), torch.LongTensor(labels)\n",
    "\n",
    "\n",
    "    def padding_and_trim(self, seq, seq_type):\n",
    "        if seq_type == 'seq':\n",
    "            seq = seq + [self.seq_mask_token]\n",
    "        seq = seq[-self.max_len:]\n",
    "        padding_len = self.max_len - len(seq)\n",
    "        seq = [0] * padding_len + seq\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyValidCollator:\n",
    "    def __init__(self, u2seq, u2answer, max_len, seq_mask_token, negative_samples, use_attributes, i2attr_map):\n",
    "        self.u2seq = u2seq\n",
    "        self.u2answer = u2answer\n",
    "        self.max_len = max_len\n",
    "        self.seq_mask_token = seq_mask_token\n",
    "        self.negative_samples = negative_samples\n",
    "        self.use_attributes = use_attributes\n",
    "        self.i2attr_map = i2attr_map\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        '''\n",
    "        batch : 1d List of tuples\n",
    "        [\n",
    "            (user, [1, 2, 3, .... (batch size)]),\n",
    "            ...\n",
    "        ]\n",
    "        '''\n",
    "        users_batch = [_[0] for _ in batch]\n",
    "        seqs_batch = [_[1] for _ in batch]\n",
    "\n",
    "        seqs_output = []\n",
    "        candidates_output = []\n",
    "        labels_output = []\n",
    "\n",
    "        for user in users_batch:  # users_batch : [user, user, .... (batch size)]\n",
    "            answer = self.u2answer[user]\n",
    "            negs = self.negative_samples[user]\n",
    "            one_seq = self.u2seq[user]\n",
    "\n",
    "            one_seq_candidates = answer + negs\n",
    "            one_seq_labels = [1] * len(answer) + [0] * len(negs)\n",
    "        \n",
    "            # seq = self.padding_and_trim(seq, 'seq')\n",
    "            one_seq = one_seq + [self.seq_mask_token]\n",
    "            one_seq = one_seq[-self.max_len:]\n",
    "            padding_len = self.max_len - len(one_seq)\n",
    "            one_seq = [0] * padding_len + one_seq\n",
    "                \n",
    "            seqs_output.append(one_seq)\n",
    "            candidates_output.append(one_seq_candidates)\n",
    "            labels_output.append(one_seq_labels)\n",
    "\n",
    "        attrs_output = []\n",
    "        if self.use_attributes:\n",
    "            for _map in self.i2attr_map:\n",
    "                one_map = []\n",
    "                for seq in seqs_output:\n",
    "                    one_map.append([_map[item] for item in seq])\n",
    "                    \n",
    "                attrs_output.append(one_map)  # attrs_output : 3d [2d, 2d, ...], 2d : one attribute\n",
    "\n",
    "        return torch.LongTensor(seqs_output), torch.LongTensor(attrs_output), torch.LongTensor(candidates_output), torch.LongTensor(labels_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers='val'\n",
    "valid_dataset = BertEvalDataset(args, train, answers, max_len,CLOZE_MASK_TOKEN, test_negative_samples, i2attr_map, use_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_valid_collator = MyValidCollator(train, val, max_len, CLOZE_MASK_TOKEN, test_negative_samples, use_attributes, i2attr_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataloader = data_utils.DataLoader(valid_dataset, batch_size=args.val_batch_size,\n",
    "    shuffle=False, pin_memory=True, collate_fn=my_valid_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in valid_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t : 2d\n",
      "^\t[  0.0000,   0.0000, ..., 1728.0000, 3707.0000]\n",
      "|\t[1107.0000, 1778.0000, ..., 421.0000, 3707.0000]\n",
      "|\t[  0.0000,   0.0000, ..., 3623.0000, 3707.0000]\n",
      "128\t\t\t...\n",
      "|\t[  0.0000,   0.0000, ..., 855.0000, 3707.0000]\n",
      "|\t[1121.0000, 1149.0000, ..., 1125.0000, 3707.0000]\n",
      "v\t[  0.0000,   0.0000, ..., 1955.0000, 3707.0000]\n",
      "\t<-- 100 -->\n",
      "\n",
      "t : 3d\n",
      "Input's shape : torch.Size([1, 128, 100])\n",
      "\\\n",
      " 1\n",
      "  \\\n",
      "  ^\t[  0.0000,   0.0000, ...,  75.0000, 302.0000]\n",
      "  |\t[ 19.0000, 254.0000, ..., 275.0000, 302.0000]\n",
      "  |\t[  0.0000,   0.0000, ..., 160.0000, 302.0000]\n",
      "  128\t\t\t...\n",
      "  |\t[  0.0000,   0.0000, ...,  33.0000, 302.0000]\n",
      "  |\t[  1.0000,  39.0000, ..., 157.0000, 302.0000]\n",
      "  v\t[  0.0000,   0.0000, ...,  74.0000, 302.0000]\n",
      "\t<-- 100 -->\n",
      "\n",
      "t : 2d\n",
      "^\t[1440.0000, 3572.0000, ..., 2118.0000, 2227.0000]\n",
      "|\t[1421.0000, 1365.0000, ..., 2856.0000, 455.0000]\n",
      "|\t[102.0000, 1930.0000, ..., 3209.0000, 2235.0000]\n",
      "128\t\t\t...\n",
      "|\t[333.0000, 2921.0000, ..., 2259.0000, 2693.0000]\n",
      "|\t[1538.0000, 400.0000, ..., 1748.0000, 1807.0000]\n",
      "v\t[467.0000, 1314.0000, ..., 166.0000, 1171.0000]\n",
      "\t<-- 101 -->\n",
      "\n",
      "t : 2d\n",
      "^\t[  1.0000,   0.0000, ...,   0.0000,   0.0000]\n",
      "|\t[  1.0000,   0.0000, ...,   0.0000,   0.0000]\n",
      "|\t[  1.0000,   0.0000, ...,   0.0000,   0.0000]\n",
      "128\t\t\t...\n",
      "|\t[  1.0000,   0.0000, ...,   0.0000,   0.0000]\n",
      "|\t[  1.0000,   0.0000, ...,   0.0000,   0.0000]\n",
      "v\t[  1.0000,   0.0000, ...,   0.0000,   0.0000]\n",
      "\t<-- 101 -->\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printer = PrintInputShape(10)\n",
    "for t in _:\n",
    "    printer.print(t, 't')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use_attributes 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_attributes = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_mask_token = CLOZE_MASK_TOKEN\n",
    "num_items = item_count\n",
    "for _ in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1050, 1340, 1565,  ...,  859, 1109,  109],\n",
       "         [   0,    0,    0,  ..., 3275, 2132, 3278],\n",
       "         [   0,    0,    0,  ..., 1821, 3395, 3342],\n",
       "         ...,\n",
       "         [ 359,  371, 3707,  ..., 2326, 3707, 2328],\n",
       "         [   0,    0,    0,  ..., 3611, 1818,  925],\n",
       "         [ 428, 1399,   21,  ...,  940, 3707, 3707]]),\n",
       " tensor([[[ 54, 294, 187,  ...,  77, 137, 275],\n",
       "          [  0,   0,   0,  ..., 160, 150, 269],\n",
       "          [  0,   0,   0,  ..., 121, 278, 216],\n",
       "          ...,\n",
       "          [102,  23, 302,  ..., 254, 302, 254],\n",
       "          [  0,   0,   0,  ..., 249, 237, 249],\n",
       "          [160, 160,  61,  ..., 263, 302, 302]]]),\n",
       " tensor([[   0,    0,    0,  ...,    0,    0,    0],\n",
       "         [   0,    0,    0,  ...,    0,    0,    0],\n",
       "         [   0,    0,    0,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   0,    0, 1274,  ...,    0, 2327,    0],\n",
       "         [   0,    0,    0,  ...,    0,    0,    0],\n",
       "         [   0,    0,    0,  ...,    0,  155, 2220]]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from model.bert_modules.embedding import TokenEmbedding, PositionalEmbedding\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Embedding which is consisted with under features\n",
    "        1. TokenEmbedding : normal embedding matrix\n",
    "        2. PositionalEmbedding : adding positional information using sin, cos\n",
    "        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n",
    "        sum of all these features are output of BERTEmbedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, max_len, use_attributes, attrs_vocab_size, i2attr_map, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: total vocab size\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(max_len=max_len, d_model=embed_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "        self.use_attributes = use_attributes\n",
    "        self.printer = PrintInputShape(5)\n",
    "\n",
    "        if use_attributes:\n",
    "            self.i2attr_map = i2attr_map\n",
    "            self.attrs_emb_mats = []\n",
    "            for size in attrs_vocab_size:\n",
    "                self.attrs_emb_mats.append(nn.Embedding(size, embed_size))\n",
    "\n",
    "    def forward(self, x, attrs):\n",
    "\n",
    "        self.printer.print(x, f\"x\")\n",
    "        self.printer.print(attrs, f\"attrs\")\n",
    "        x = self.token(x) + self.position(x) # now x's shape : Batch X seq_len X emb size (3d)\n",
    "        \n",
    "        if self.use_attributes:\n",
    "            attr_sum = torch.zeros_like(x)\n",
    "            self.printer.print(attr_sum, f\"attr_sum before sum\")\n",
    "            for i, (at_idx, one_attr_mat) in enumerate(zip(attrs, self.attrs_emb_mats)):\n",
    "                self.printer.print(one_attr_mat(at_idx), f\"one_attr_mat(at_idx)\")\n",
    "                attr_sum += one_attr_mat(at_idx)\n",
    "            self.printer.print(attr_sum, f\"attr_sum after sum\")\n",
    "            x += attr_sum\n",
    "\n",
    "        # x's shape : Batch X seq_len (2d)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model.bert import BERT\n",
    "from torch import nn as nn\n",
    "\n",
    "# from model.bert_modules.embedding import BERTEmbedding\n",
    "from model.bert_modules.transformer import TransformerBlock\n",
    "from utils import fix_random_seed_as\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, args, i2attr_map):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        fix_random_seed_as(args.model_init_seed)\n",
    "        # self.init_weights()\n",
    "\n",
    "        max_len = args.bert_max_len\n",
    "        num_items = args.num_items\n",
    "        attrs_each_size = args.attrs_each_size\n",
    "        attrs_vocab_size = [_ + 2 for _ in attrs_each_size]\n",
    "        n_layers = args.bert_num_blocks\n",
    "        heads = args.bert_num_heads\n",
    "        vocab_size = num_items + 2\n",
    "        if args.use_attributes:\n",
    "            use_attributes = args.use_attributes\n",
    "    \n",
    "        hidden = args.bert_hidden_units\n",
    "        self.hidden = hidden\n",
    "        dropout = args.bert_dropout\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=self.hidden, max_len=max_len, \n",
    "        use_attributes=use_attributes, attrs_vocab_size=attrs_vocab_size, i2attr_map=i2attr_map, dropout=dropout)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden, heads, hidden * 4, dropout) for _ in range(n_layers)])\n",
    "\n",
    "        self.out = nn.Linear(self.hidden, num_items + 1)\n",
    "\n",
    "    def forward(self, x, attrs):\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x, attrs)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "\n",
    "        return self.out(x)\n",
    "\n",
    "    def init_weights(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT(args, i2attr_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import AverageMeterSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.num_epochs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer.utils import recalls_and_ndcgs_for_ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 7.809\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m batch \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m batch]\n\u001b[1;32m     24\u001b[0m seqs, attrs, candidates, labels \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> 25\u001b[0m scores \u001b[39m=\u001b[39m model(seqs, attrs)  \u001b[39m# B x T x V\u001b[39;00m\n\u001b[1;32m     26\u001b[0m scores \u001b[39m=\u001b[39m scores[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]  \u001b[39m# B x V\u001b[39;00m\n\u001b[1;32m     27\u001b[0m scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mgather(\u001b[39m1\u001b[39m, candidates)  \u001b[39m# B x C\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.9/envs/candidate/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[73], line 48\u001b[0m, in \u001b[0;36mBERT.forward\u001b[0;34m(self, x, attrs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m# running over multiple transformer blocks\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m transformer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_blocks:\n\u001b[0;32m---> 48\u001b[0m     x \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mforward(x, mask)\n\u001b[1;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(x)\n",
      "File \u001b[0;32m~/PycharmProjects/BertCustom/BertCustom/model/bert_modules/transformer.py:31\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, mask):\n\u001b[1;32m     30\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_sublayer(x, \u001b[39mlambda\u001b[39;00m _x: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention\u001b[39m.\u001b[39mforward(_x, _x, _x, mask\u001b[39m=\u001b[39mmask))\n\u001b[0;32m---> 31\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_sublayer(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward)\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.9/envs/candidate/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PycharmProjects/BertCustom/BertCustom/model/bert_modules/utils.py:20\u001b[0m, in \u001b[0;36mSublayerConnection.forward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, sublayer):\n\u001b[1;32m     19\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mApply residual connection to any sublayer with the same size.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(sublayer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm(x)))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.9/envs/candidate/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PycharmProjects/BertCustom/BertCustom/model/bert_modules/utils.py:34\u001b[0m, in \u001b[0;36mPositionwiseFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw_1(x))))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.9/envs/candidate/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.9/envs/candidate/lib/python3.9/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.9/envs/candidate/lib/python3.9/site-packages/torch/nn/functional.py:1279\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1279\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "average_meter_set = AverageMeterSet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "ce = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = [x.to(args.device) for x in batch]\n",
    "        seqs, attrs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(seqs, attrs)\n",
    "        logits = logits.view(-1, logits.size(-1))  # (B*T) x V\n",
    "        labels = labels.view(-1)  # B*T\n",
    "        loss = ce(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        average_meter_set.update('loss', loss.item())\n",
    "    print(f\"Epoch {epoch}, loss {average_meter_set['loss'].avg:.3f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(valid_dataloader):\n",
    "            batch = [x.to(args.device) for x in batch]\n",
    "            seqs, attrs, candidates, labels = batch\n",
    "            scores = model(seqs, attrs)  # B x T x V\n",
    "            scores = scores[:, -1, :]  # B x V\n",
    "            scores = scores.gather(1, candidates)  # B x C\n",
    "\n",
    "            metrics = recalls_and_ndcgs_for_ks(scores, labels, args.metric_ks)\n",
    "            for k, v in metrics.items():\n",
    "                average_meter_set.update(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.059370889832886554"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_meter_set.meters['NDCG@10'].avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(test_negative_samples.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6039"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_negative_samples.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintInputShape:\n",
    "    def __init__(self, limit=1):\n",
    "        self.limit = limit\n",
    "        self.cnt = 1\n",
    "\n",
    "    # @classmethod\n",
    "    def print(self, t:np.array, notation):\n",
    "        if self.cnt <= self.limit:\n",
    "            print(f\"{notation} : {len(t.shape)}d\", end='\\n')\n",
    "            if len(t.shape) == 1:\n",
    "                self.print_1d(t)\n",
    "                self.print_0d_shape(len(t))\n",
    "            elif len(t.shape) == 2:\n",
    "                self.print_2d(t)\n",
    "            elif len(t.shape) == 3:\n",
    "                self.print_3d(t)\n",
    "            else:\n",
    "                print(f\"1d array or over 3d\")\n",
    "            print()\n",
    "        self.cnt += 1\n",
    "\n",
    "    def print_0d_shape(self, dim, ws=''):\n",
    "        print(f\"{ws}<-- {dim} -->\")\n",
    "\n",
    "    def print_1d(self, data_1d, ws='', arrow=''):\n",
    "        if data_1d[0] % int(data_1d[0]):\n",
    "            sample = f\"[{data_1d[0]:8.4f}, {data_1d[1]:8.4f}, {data_1d[2]:8.4f}, ..., {data_1d[-3]:8.4f}, {data_1d[-2]:8.4f}, {data_1d[-1]:8.4f}]\"\n",
    "        else:\n",
    "            sample = f\"[{data_1d[0]:8}, {data_1d[1]:8}, {data_1d[2]:8}, ..., {data_1d[-3]:8}, {data_1d[-2]:8}, {data_1d[-1]:8}]\"\n",
    "        print(f\"{ws}{arrow}\\t{sample}\")\n",
    "        \n",
    "\n",
    "    def print_2d(self, t, ws=''):\n",
    "        t_trim_a = t.clone().detach()[:3]\n",
    "        t_trim_b = t.clone().detach()[-3:]\n",
    "        t_trim = np.concatenate([t_trim_a, t_trim_b], axis=0)\n",
    "        for line, data_1d in enumerate(t_trim, start=1):\n",
    "            if line == 1:\n",
    "                self.print_1d(data_1d, ws, arrow='^')\n",
    "            elif line == 2:\n",
    "                self.print_1d(data_1d, ws, arrow='|')\n",
    "            elif line == (len(t_trim)-1):\n",
    "                self.print_1d(data_1d, ws, arrow='|')\n",
    "            elif line == len(t_trim):\n",
    "                self.print_1d(data_1d, ws, arrow='v')\n",
    "            else:\n",
    "                self.print_1d(data_1d, ws, arrow='|')\n",
    "\n",
    "            if line == len(t_trim)//2:  # 중간에 숫자 끼워넣기\n",
    "                print(f\"{ws}{t.size()[0]}\\t\\t\\t...\")\n",
    "        self.print_0d_shape(t.size()[1], ws)\n",
    "\n",
    "    def print_3d(self, t):\n",
    "        print(f\"Input's shape : {t.size()}\")\n",
    "        ws = ''\n",
    "        print(f\"{ws}\\\\\")\n",
    "        ws += ' '\n",
    "        print(f\"{ws}{t.size()[0]}\")\n",
    "        ws += ' '\n",
    "        print(f\"{ws}\\\\\")\n",
    "\n",
    "        self.print_2d(t[-1], ws=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset(use_attributes=False):\n",
    "    if use_attributes:\n",
    "        return [1, 2, 3], [4, 5, 6]\n",
    "    else:\n",
    "        return [1, 2, 3], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward(x):\n",
    "\n",
    "    seq, attrs = x\n",
    "\n",
    "    print(seq)\n",
    "    print(attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3], [])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = test_dataset()\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "test_forward(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, *c = ( [4, 5, 6], [7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {1 : [1, 2,3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: [1, 2, 3]}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a\u001b[39m.\u001b[39;49mappend(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'append'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "candidate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c9a439ae93980d4049c8b7e2fc8dfa6026c40a268f63cc6d8c2833fbd13c5bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
